/**
 * ä¸ºä¸­å›½å¼€å‘è€…ä¼˜åŒ–çš„ API é¢„è®¾é…ç½®
 * ä¸“æ³¨äºå„ç±»ä¸­è½¬ç«™å’Œæœ¬åœ°éƒ¨ç½²
 */

export const CHINA_API_PRESETS = {
  'ğŸš€ é€šç”¨ä¸­è½¬ç«™': {
    endpoint: '',
    format: 'openai',
    placeholder: 'sk-xxxxxxxxxxxxxx',
    defaultModel: 'gpt-3.5-turbo',
    description: 'æ”¯æŒ OpenAI æ ¼å¼çš„é€šç”¨ä¸­è½¬ç«™',
    configTemplate: {
      baseUrl: 'https://api.your-proxy.com',
      models: ['gpt-3.5-turbo', 'gpt-4', 'claude-3-sonnet'],
      needsProxy: false
    }
  },
  
  'ğŸŒŸ OneAPI ä¸­è½¬': {
    endpoint: 'https://api.oneapi.com/v1/chat/completions',
    format: 'openai',
    placeholder: 'sk-oneapi-xxxxxx',
    defaultModel: 'gpt-3.5-turbo',
    description: 'OneAPI ç»Ÿä¸€æ¥å£ï¼Œæ”¯æŒå¤šç§æ¨¡å‹',
    configTemplate: {
      supportedModels: ['gpt-3.5-turbo', 'gpt-4', 'claude-3', 'gemini-pro'],
      features: ['balance-query', 'model-list', 'usage-stats']
    }
  },
  
  'ğŸ§ è…¾è®¯æ··å…ƒ': {
    endpoint: 'https://hunyuan.cloud.tencent.com/v1/chat/completions',
    format: 'openai',
    placeholder: 'secret_id:secret_key',
    defaultModel: 'hunyuan-lite',
    description: 'è…¾è®¯æ··å…ƒå¤§æ¨¡å‹ï¼Œå›½äº§ä¹‹å…‰',
    configTemplate: {
      models: ['hunyuan-lite', 'hunyuan-standard', 'hunyuan-pro'],
      region: 'ap-guangzhou'
    }
  },
  
  'ğŸŒ™ æœˆä¹‹æš—é¢ Kimi': {
    endpoint: 'https://api.moonshot.cn/v1/chat/completions',
    format: 'openai',
    placeholder: 'sk-xxxxxxxxxxxxxx',
    defaultModel: 'moonshot-v1-8k',
    description: 'Kimi AIï¼Œè¶…é•¿ä¸Šä¸‹æ–‡æ”¯æŒ',
    configTemplate: {
      models: ['moonshot-v1-8k', 'moonshot-v1-32k', 'moonshot-v1-128k'],
      maxContext: 128000
    }
  },
  
  'ğŸ”¥ æ™ºè°± GLM': {
    endpoint: 'https://open.bigmodel.cn/api/paas/v4/chat/completions',
    format: 'openai',
    placeholder: 'your-api-key',
    defaultModel: 'glm-4',
    description: 'æ™ºè°±æ¸…è¨€ï¼Œä¸­æ–‡ç†è§£èƒ½åŠ›å¼º',
    configTemplate: {
      models: ['glm-4', 'glm-3-turbo', 'glm-4v'],
      features: ['vision', 'function-calling']
    }
  },
  
  'ğŸ  æœ¬åœ° Ollama': {
    endpoint: 'http://localhost:11434/v1/chat/completions',
    format: 'openai',
    placeholder: 'æ— éœ€å¯†é’¥',
    defaultModel: 'qwen:7b',
    description: 'æœ¬åœ°è¿è¡Œçš„å¼€æºæ¨¡å‹',
    configTemplate: {
      models: ['llama2', 'qwen', 'chatglm3', 'mistral'],
      isLocal: true,
      requiresGPU: true
    }
  },
  
  'ğŸŒ è‡ªå®šä¹‰ç«¯ç‚¹': {
    endpoint: '',
    format: 'custom',
    placeholder: 'è¾“å…¥æ‚¨çš„ API å¯†é’¥',
    defaultModel: '',
    description: 'é…ç½®æ‚¨è‡ªå·±çš„ API ç«¯ç‚¹',
    configTemplate: {
      supportedFormats: ['openai', 'anthropic', 'google'],
      customHeaders: {},
      timeout: 30000
    }
  }
}

// API æ ¼å¼æ£€æµ‹è§„åˆ™
export const API_FORMAT_RULES = {
  openai: {
    test: (url) => url.includes('/chat/completions') || url.includes('/v1/'),
    requestAdapter: (messages, config) => ({
      model: config.model,
      messages: messages,
      stream: config.stream || false,
      temperature: config.temperature,
      max_tokens: config.maxTokens,
      top_p: config.topP,
      frequency_penalty: config.frequencyPenalty,
      presence_penalty: config.presencePenalty,
    }),
    responseParser: (data) => {
      if (data.choices && data.choices[0]) {
        return data.choices[0].message.content
      }
      throw new Error('Invalid response format')
    }
  },
  
  anthropic: {
    test: (url) => url.includes('anthropic') || url.includes('/messages'),
    requestAdapter: (messages, config) => ({
      model: config.model,
      messages: messages.filter(m => m.role !== 'system'),
      system: messages.find(m => m.role === 'system')?.content,
      max_tokens: config.maxTokens || 1024,
      temperature: config.temperature,
      top_p: config.topP,
    }),
    responseParser: (data) => {
      if (data.content && data.content[0]) {
        return data.content[0].text
      }
      throw new Error('Invalid response format')
    }
  },
  
  google: {
    test: (url) => url.includes('google') || url.includes('generativelanguage'),
    requestAdapter: (messages, config) => ({
      contents: messages.map(m => ({
        role: m.role === 'assistant' ? 'model' : 'user',
        parts: [{ text: m.content }]
      })),
      generationConfig: {
        temperature: config.temperature,
        maxOutputTokens: config.maxTokens,
        topP: config.topP,
        topK: config.topK,
      }
    }),
    responseParser: (data) => {
      if (data.candidates && data.candidates[0]) {
        return data.candidates[0].content.parts[0].text
      }
      throw new Error('Invalid response format')
    }
  }
}

// æ¨¡å‹èƒ½åŠ›æ ‡ç­¾
export const MODEL_CAPABILITIES = {
  'text': 'æ–‡æœ¬ç”Ÿæˆ',
  'vision': 'å›¾åƒç†è§£',
  'function-calling': 'å‡½æ•°è°ƒç”¨',
  'code': 'ä»£ç ç”Ÿæˆ',
  'math': 'æ•°å­¦æ¨ç†',
  'long-context': 'é•¿æ–‡æœ¬',
  'multilingual': 'å¤šè¯­è¨€',
  'fast': 'å¿«é€Ÿå“åº”',
  'cheap': 'ç»æµå®æƒ ',
  'free': 'å…è´¹ä½¿ç”¨'
}

// å‚æ•°é…ç½®æ¨¡æ¿
export const PARAMETER_TEMPLATES = {
  creative: {
    name: 'åˆ›æ„æ¨¡å¼',
    temperature: 0.9,
    topP: 0.95,
    presencePenalty: 0.5,
    frequencyPenalty: 0.5,
    description: 'æ›´æœ‰åˆ›é€ åŠ›å’Œæƒ³è±¡åŠ›çš„å›å¤'
  },
  
  balanced: {
    name: 'å¹³è¡¡æ¨¡å¼',
    temperature: 0.7,
    topP: 0.9,
    presencePenalty: 0,
    frequencyPenalty: 0,
    description: 'å¹³è¡¡åˆ›é€ æ€§å’Œå‡†ç¡®æ€§'
  },
  
  precise: {
    name: 'ç²¾ç¡®æ¨¡å¼',
    temperature: 0.3,
    topP: 0.5,
    presencePenalty: 0,
    frequencyPenalty: 0,
    description: 'æ›´å‡†ç¡®å’Œä¸€è‡´çš„å›å¤'
  },
  
  deterministic: {
    name: 'ç¡®å®šæ¨¡å¼',
    temperature: 0,
    topP: 1,
    presencePenalty: 0,
    frequencyPenalty: 0,
    description: 'æ¯æ¬¡éƒ½è¿”å›ç›¸åŒçš„ç»“æœ'
  }
}